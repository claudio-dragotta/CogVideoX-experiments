{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogVideoX - Text-to-Video Generation & Experiments\n",
    "\n",
    "This notebook demonstrates **inference** with [CogVideoX-2b](https://huggingface.co/THUDM/CogVideoX-2b) and runs **ablation studies** on key hyperparameters.\n",
    "\n",
    "**Model:** CogVideoX-2b (THUDM) — 2B parameter text-to-video diffusion model  \n",
    "**Runtime:** Google Colab with T4/A100 GPU  \n",
    "\n",
    "**Experiments:**\n",
    "1. Ablation on `guidance_scale` (1, 6, 12) — prompt adherence strength\n",
    "2. Ablation on `num_inference_steps` (10, 25, 50) — quality vs speed\n",
    "3. Seed variation (42, 123, 999) — generation diversity\n",
    "4. Frame count variation (25, 49) — video length vs VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers>=0.30.1 transformers>=4.44.2 accelerate>=0.33.0 imageio-ffmpeg>=0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import CogVideoXPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "from IPython.display import Video, display, HTML\n",
    "\n",
    "print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function\n",
    "Utility to run a generation experiment and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(pipe, prompt, filename, steps=50, frames=49, guidance=6, seed=42, fps=8):\n",
    "    \"\"\"Generate a video and return timing + key frames.\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    start = time.time()\n",
    "    video_frames = pipe(\n",
    "        prompt=prompt,\n",
    "        num_videos_per_prompt=1,\n",
    "        num_inference_steps=steps,\n",
    "        num_frames=frames,\n",
    "        guidance_scale=guidance,\n",
    "        generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n",
    "    ).frames[0]\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    export_to_video(video_frames, filename, fps=fps)\n",
    "\n",
    "    n = len(video_frames)\n",
    "    key_frames = [\n",
    "        np.array(video_frames[0]),\n",
    "        np.array(video_frames[n // 2]),\n",
    "        np.array(video_frames[-1]),\n",
    "    ]\n",
    "\n",
    "    print(f\"  -> {elapsed:.1f}s | {filename}\")\n",
    "    return {\"time_s\": elapsed, \"file\": filename, \"key_frames\": key_frames}\n",
    "\n",
    "\n",
    "def plot_comparison(results, labels, param_name, title):\n",
    "    \"\"\"Plot key frames grid and timing bar chart.\"\"\"\n",
    "    n_runs = len(results)\n",
    "    fig, axes = plt.subplots(n_runs, 3, figsize=(14, 4.5 * n_runs))\n",
    "    if n_runs == 1:\n",
    "        axes = [axes]\n",
    "    frame_names = [\"First Frame\", \"Middle Frame\", \"Last Frame\"]\n",
    "    for i, (res, label) in enumerate(zip(results, labels)):\n",
    "        for j, (frame, fn) in enumerate(zip(res[\"key_frames\"], frame_names)):\n",
    "            axes[i][j].imshow(frame)\n",
    "            axes[i][j].set_title(f\"{label} - {fn}\", fontsize=11)\n",
    "            axes[i][j].axis(\"off\")\n",
    "    plt.suptitle(title, fontsize=14, y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Timing chart\n",
    "    fig, ax = plt.subplots(figsize=(7, 3.5))\n",
    "    colors = [\"#3498db\", \"#2ecc71\", \"#e74c3c\", \"#f39c12\"][:n_runs]\n",
    "    times = [r[\"time_s\"] for r in results]\n",
    "    bars = ax.bar(labels, times, color=colors)\n",
    "    ax.set_ylabel(\"Time (s)\")\n",
    "    ax.set_title(f\"Inference Time vs {param_name}\")\n",
    "    for bar, t in zip(bars, times):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n",
    "                f\"{t:.1f}s\", ha=\"center\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Helpers loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = CogVideoXPipeline.from_pretrained(\n",
    "    \"THUDM/CogVideoX-2b\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.vae.enable_tiling()\n",
    "pipe.vae.enable_slicing()\n",
    "\n",
    "print(\"CogVideoX-2b loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Inference — Baseline Video\n",
    "\n",
    "Single video generation with default parameters to demonstrate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool \"\n",
    "    \"in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, \"\n",
    "    \"producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously \"\n",
    "    \"and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle \"\n",
    "    \"glow on the scene. The panda's face is expressive, showing concentration and joy as it \"\n",
    "    \"plays. The background includes a small, flowing stream and vibrant green foliage, \"\n",
    "    \"enhancing the peaceful and magical atmosphere of this unique musical performance.\"\n",
    ")\n",
    "\n",
    "print(\"Generating baseline video (steps=50, GS=6, seed=42, frames=49)...\")\n",
    "baseline = run_experiment(pipe, prompt, \"output_baseline.mp4\")\n",
    "Video(\"output_baseline.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Experiment 1 — Ablation on `guidance_scale`\n",
    "\n",
    "The `guidance_scale` (classifier-free guidance) controls prompt adherence:\n",
    "- **GS=1:** creative/random, weak conditioning\n",
    "- **GS=6:** balanced quality and adherence (default)\n",
    "- **GS=12:** strong adherence, risk of artifacts\n",
    "\n",
    "All other params fixed: steps=50, seed=42, frames=49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_values = [1, 6, 12]\n",
    "gs_results = []\n",
    "\n",
    "for gs in gs_values:\n",
    "    print(f\"guidance_scale={gs}\")\n",
    "    res = run_experiment(pipe, prompt, f\"output_gs{gs}.mp4\", guidance=gs)\n",
    "    res[\"param\"] = gs\n",
    "    gs_results.append(res)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(gs_results, [f\"GS={r['param']}\" for r in gs_results],\n",
    "               \"guidance_scale\", \"Experiment 1: guidance_scale Ablation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in gs_results:\n",
    "    print(f\"\\nguidance_scale={r['param']} ({r['time_s']:.1f}s)\")\n",
    "    display(Video(r[\"file\"], embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Experiment 2 — Ablation on `num_inference_steps`\n",
    "\n",
    "The number of diffusion steps directly affects quality and speed:\n",
    "- **10 steps:** fast but noisy/grainy output\n",
    "- **25 steps:** good compromise between speed and quality\n",
    "- **50 steps:** maximum quality (default), slowest\n",
    "\n",
    "All other params fixed: GS=6, seed=42, frames=49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_values = [10, 25, 50]\n",
    "step_results = []\n",
    "\n",
    "for s in step_values:\n",
    "    print(f\"steps={s}\")\n",
    "    res = run_experiment(pipe, prompt, f\"output_steps{s}.mp4\", steps=s)\n",
    "    res[\"param\"] = s\n",
    "    step_results.append(res)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(step_results, [f\"Steps={r['param']}\" for r in step_results],\n",
    "               \"num_inference_steps\", \"Experiment 2: Inference Steps Ablation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in step_results:\n",
    "    print(f\"\\nsteps={r['param']} ({r['time_s']:.1f}s)\")\n",
    "    display(Video(r[\"file\"], embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Experiment 3 — Seed Variation\n",
    "\n",
    "Different seeds produce different videos from the same prompt, demonstrating generation diversity.\n",
    "We use three seeds (42, 123, 999) with all other params at default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_values = [42, 123, 999]\n",
    "seed_results = []\n",
    "\n",
    "for sd in seed_values:\n",
    "    print(f\"seed={sd}\")\n",
    "    res = run_experiment(pipe, prompt, f\"output_seed{sd}.mp4\", seed=sd)\n",
    "    res[\"param\"] = sd\n",
    "    seed_results.append(res)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(seed_results, [f\"Seed={r['param']}\" for r in seed_results],\n",
    "               \"seed\", \"Experiment 3: Seed Variation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in seed_results:\n",
    "    print(f\"\\nseed={r['param']} ({r['time_s']:.1f}s)\")\n",
    "    display(Video(r[\"file\"], embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Experiment 4 — Frame Count Variation\n",
    "\n",
    "More frames produce longer videos but require more VRAM and time.\n",
    "We compare 25 frames (~3s) vs 49 frames (~6s) at 8 FPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_values = [25, 49]\n",
    "frame_results = []\n",
    "\n",
    "for f in frame_values:\n",
    "    print(f\"frames={f}\")\n",
    "    res = run_experiment(pipe, prompt, f\"output_frames{f}.mp4\", frames=f)\n",
    "    res[\"param\"] = f\n",
    "    frame_results.append(res)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(frame_results, [f\"Frames={r['param']}\" for r in frame_results],\n",
    "               \"num_frames\", \"Experiment 4: Frame Count Variation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in frame_results:\n",
    "    print(f\"\\nframes={r['param']} ({r['time_s']:.1f}s)\")\n",
    "    display(Video(r[\"file\"], embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Overall Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "experiments = [\n",
    "    (\"Exp 1: guidance_scale\", gs_results, \"guidance_scale\"),\n",
    "    (\"Exp 2: inference_steps\", step_results, \"steps\"),\n",
    "    (\"Exp 3: seed\", seed_results, \"seed\"),\n",
    "    (\"Exp 4: num_frames\", frame_results, \"frames\"),\n",
    "]\n",
    "\n",
    "for name, res_list, param in experiments:\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * 45)\n",
    "    for r in res_list:\n",
    "        print(f\"  {param}={r['param']:<8} -> {r['time_s']:.1f}s  ({r['file']})\")\n",
    "\n",
    "total_runs = sum(len(r) for _, r, _ in experiments) + 1  # +1 baseline\n",
    "total_time = baseline[\"time_s\"] + sum(r[\"time_s\"] for _, res, _ in experiments for r in res)\n",
    "print(f\"\\nTotal runs: {total_runs}\")\n",
    "print(f\"Total generation time: {total_time/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Conclusions\n",
    "\n",
    "### guidance_scale (Experiment 1)\n",
    "- **GS=1**: Blurry, unfocused, model ignores the prompt.\n",
    "- **GS=6** (default): Best balance of quality and prompt adherence.\n",
    "- **GS=12**: Over-conditioned, oversaturated colors and artifacts.\n",
    "- **Impact on time**: Minimal — generation time is dominated by diffusion steps, not guidance strength.\n",
    "\n",
    "### num_inference_steps (Experiment 2)\n",
    "- **10 steps**: Fast (~5x speedup) but noisy, grainy output with visible artifacts.\n",
    "- **25 steps**: Good compromise — noticeable quality improvement over 10, much faster than 50.\n",
    "- **50 steps** (default): Best quality, slowest generation.\n",
    "- **Impact on time**: Linear — halving steps roughly halves generation time.\n",
    "\n",
    "### seed (Experiment 3)\n",
    "- Different seeds produce **visually distinct videos** from the same prompt.\n",
    "- Composition, colors, and motion patterns vary significantly.\n",
    "- **Impact on time**: None — seed does not affect generation speed.\n",
    "\n",
    "### num_frames (Experiment 4)\n",
    "- **25 frames** (~3s video): Faster, less VRAM, but very short output.\n",
    "- **49 frames** (~6s video): Default, longer and more coherent motion.\n",
    "- **Impact on time**: Roughly proportional — fewer frames = faster generation.\n",
    "\n",
    "### Key Takeaway\n",
    "The two most impactful hyperparameters are **`guidance_scale`** (quality/adherence trade-off) and **`num_inference_steps`** (quality/speed trade-off). The default values (GS=6, steps=50) are well-chosen for maximum quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
